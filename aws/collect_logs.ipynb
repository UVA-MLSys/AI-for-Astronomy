{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T15:59:15.052536Z",
     "start_time": "2025-09-04T15:59:15.050318Z"
    }
   },
   "source": [
    "import boto3, re, json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T15:59:17.695447Z",
     "start_time": "2025-09-04T15:59:17.692987Z"
    }
   },
   "source": [
    "result_path_pattern = re.compile(r\"result-partition-(?P<partition>[0-9]+MB)/(?P<data_size>[0-9]+GB)/(run )?(?P<run_no>[0-9]+)\")\n",
    "search_result = result_path_pattern.search(\"result-partition-75MB/1GB/1\")\n",
    "search_dict = search_result.groupdict()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"result-partition-[0-9]+MB/((total)|([0-9]+GB))/(run )?[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:17:59.147469Z",
     "start_time": "2025-09-04T16:17:59.131188Z"
    }
   },
   "source": "def get_event_details(event):\n    return event.get('executionSucceededEventDetails') or \\\n        event.get('executionFailedEventDetails') or \\\n        event.get('executionStartedEventDetails') or \\\n        event.get('stateEnteredEventDetails') or {}\n\ndef extract_event_details(history, results, verbose):\n    state = 0\n    execution_start, execution_end = None, None\n    inference_start, inference_end = None, None\n    result_path = None\n\n    # Print execution events\n    for event in history['events']:\n        timestamp = event['timestamp']\n        event_type = event['type']\n        details = get_event_details(event)\n        \n        if verbose:print(f\"{timestamp} - {event_type}\")\n        if details is not None and verbose:\n            print(f\"  Details: {details}\")\n            \n        try:\n            if event_type == 'ExecutionStarted': \n                execution_start = timestamp\n            elif event_type == 'ExecutionSucceeded': execution_end = timestamp\n            elif event_type == 'TaskStateEntered': \n                if state ==0:\n                    input_json = json.loads(details['input'])\n                    result_path = input_json['result_path']\n                    # print(f\"{timestamp}: Result Path: {result_path}\")\n                \n                    # if pattern.fullmatch(result_path) is None:\n                    if \"demo\" in result_path or result_path in results['result_path']:\n                        # print(f\"Skipping {result_path}.\")\n                        return results\n                    \n                    result_path_splitted = result_path.split('/')\n                    # \"result-partition-100MB/1GB/1\" or \"result-partition-100MB/1GB/run 1\"\n                    \n                    if 'Batches' in result_path:\n                        data_size = result_path_splitted[-4]\n                    else:\n                        data_size = result_path_splitted[-2]\n                    if data_size == 'total': data_size = '12.6GB'\n                    \n                    run_no = result_path_splitted[-1].replace('run ', '')\n                    partition_size = input_json['data_prefix']\n                    batch_size = input_json['batch_size']\n                \n                state += 1\n            elif event_type == 'MapStateEntered':\n                # input_json = json.loads(details['input'])\n                 \n                inference_start = timestamp\n                state += 1\n            elif event_type == 'MapStateExited': \n                inference_end = timestamp\n                state += 1\n                \n        except Exception as e:\n            print(f\"  Error {e.with_traceback}\")\n            return results\n            \n    if verbose: print(\"-\" * 80)\n    \n    delta = (execution_end - execution_start)\n    total_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n    if verbose: print(f\"Total Duration: {total_duration:.2f} seconds.\")\n\n    delta = (inference_end - inference_start)\n    inference_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n    if verbose: print(f\"Inference Duration: {inference_duration:.2f} seconds.\")\n\n    results['result_path'].append(result_path)\n    results['data (GB)'].append(data_size)\n    results['run'].append(run_no)\n    results['partition (MB)'].append(partition_size)\n    results['total_duration (s)'].append(total_duration)\n    results['inference_duration (s)'].append(inference_duration)\n    results['batch_size'].append(batch_size)\n    results['batch_varying'].append('Batches' in result_path)\n    \n    return results\n\ndef get_step_function_logs(\n    state_machine_arn, start_date=None, end_date=None,\n    verbose=False, profile_name=None\n):\n    \"\"\"\n    Collects the events log for a specific AWS Step Functions state machine.\n\n    :param state_machine_arn: The ARN of the Step Functions state machine\n    :param start_date: Optional, filter logs starting from this date (timezone-aware datetime object)\n    :param end_date: Optional, filter logs until this date (timezone-aware datetime object)\n    :param profile_name: Optional, AWS profile name to use\n    \"\"\"\n    # Initialize boto3 session with profile if provided\n    if profile_name:\n        session = boto3.Session(profile_name=profile_name)\n        stepfunctions_client = session.client('stepfunctions', region_name='us-east-1')\n    else:\n        stepfunctions_client = boto3.client('stepfunctions', region_name='us-east-1')\n        \n    results = {\n        key: [] for key in [\n            'result_path', 'data (GB)','batch_size', 'run', \n            'partition (MB)', 'total_duration (s)', \n            'inference_duration (s)', 'batch_varying']\n    }\n\n    try:\n        # List executions for the state machine\n        executions = stepfunctions_client.list_executions(\n            stateMachineArn=state_machine_arn,\n            statusFilter='SUCCEEDED',  # You can filter by RUNNING, FAILED, etc.\n            maxResults=1000 # update if you have more than 1000 executions\n        )\n\n        print(f\"Fetching logs for Step Functions state machine: {state_machine_arn}\\n\")\n        print(f\"Found {len(executions['executions'])} executions.\\n\")\n\n        # Iterate through executions\n        for execution in executions['executions']:\n            execution_arn = execution['executionArn']\n            start_time = execution['startDate']\n            stop_time = execution['stopDate']\n            \n            # Ensure start_date and end_date are timezone-aware and in UTC\n            if start_date:\n                start_date = start_date.astimezone(pytz.utc)\n            if end_date:\n                end_date = end_date.astimezone(pytz.utc)\n\n            # Filter by start_date and end_date if provided\n            if start_date and start_time < start_date:\n                continue\n            if end_date and stop_time > end_date:\n                continue\n            \n            if verbose: print(f\"Execution ARN: {execution_arn}\")\n            if verbose: print(f\"Start Time: {start_time}, Stop Time: {stop_time}\")\n\n            # Get execution history\n            history = stepfunctions_client.get_execution_history(\n                executionArn=execution_arn,\n                reverseOrder=False  # Set to True if you want events in reverse order\n            )\n            \n            results = extract_event_details(history, results, verbose)\n            # break\n            \n    except stepfunctions_client.exceptions.ResourceNotFound:\n        print(f\"The state machine ARN {state_machine_arn} does not exist.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        \n    del results['result_path']\n    return results",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:18:11.668291Z",
     "start_time": "2025-09-04T16:18:07.220591Z"
    }
   },
   "source": [
    "state_machine_arn = \"arn:aws:states:us-east-1:448324707516:stateMachine:DataParallel-CosmicAI\"\n",
    "\n",
    "# Optional: Define a time range (use timezone-aware UTC datetimes)\n",
    "start_date = datetime(2024, 11, 11, 0, 0, 0, tzinfo=pytz.utc)  # Example: Start from this date\n",
    "end_date = None # datetime(2024, 11, 22, 23, 0, 0, tzinfo=pytz.utc)    # Example: Until this date\n",
    "\n",
    "results = get_step_function_logs(state_machine_arn, start_date, end_date, profile_name= \"cylon\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching logs for Step Functions state machine: arn:aws:states:us-east-1:448324707516:stateMachine:DataParallel-CosmicAI\n",
      "\n",
      "Found 16 executions.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:18:28.386832Z",
     "start_time": "2025-09-04T16:18:28.358908Z"
    }
   },
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df[(df['data (GB)'] != '100MB') & (df['partition (MB)'] != 'data')]\n",
    "# because the first ones are latest\n",
    "df.drop_duplicates(subset=['data (GB)', 'batch_size', 'run', 'partition (MB)', 'batch_varying'], inplace=True, keep='first')\n",
    "df.head(3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  data (GB)  batch_size run partition (MB)  total_duration (s)  \\\n",
       "0     983GB         512   3          100MB             108.307   \n",
       "1     983GB         512   2          100MB             126.489   \n",
       "2     983GB         512   1          100MB             122.324   \n",
       "\n",
       "   inference_duration (s)  batch_varying  \n",
       "0                  58.785          False  \n",
       "1                  76.963          False  \n",
       "2                  67.522          False  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data (GB)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>run</th>\n",
       "      <th>partition (MB)</th>\n",
       "      <th>total_duration (s)</th>\n",
       "      <th>inference_duration (s)</th>\n",
       "      <th>batch_varying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100MB</td>\n",
       "      <td>108.307</td>\n",
       "      <td>58.785</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100MB</td>\n",
       "      <td>126.489</td>\n",
       "      <td>76.963</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100MB</td>\n",
       "      <td>122.324</td>\n",
       "      <td>67.522</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:18:34.556803Z",
     "start_time": "2025-09-04T16:18:34.548185Z"
    }
   },
   "source": [
    "df['data (GB)'] = df['data (GB)'].str.replace('GB', '').astype(float)\n",
    "df['partition (MB)'] = df['partition (MB)'].str.replace('MB', '').astype(int)\n",
    "df['num_worlds'] = ((df['data (GB)'] * 1024 + df['partition (MB)'] -1 ) // df['partition (MB)']).astype(int)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:18:38.434311Z",
     "start_time": "2025-09-04T16:18:38.423253Z"
    }
   },
   "source": [
    "df = df[['partition (MB)', 'data (GB)', 'batch_size', 'batch_varying', 'run', 'num_worlds', 'total_duration (s)', 'inference_duration (s)']]\n",
    "df.sort_values(by=['partition (MB)', 'data (GB)','batch_size','batch_varying', 'run'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:18:44.148901Z",
     "start_time": "2025-09-04T16:18:44.137155Z"
    }
   },
   "source": [
    "df.round(2).to_csv('./results/state_machine_logs.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
